{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53fe7042-dcd9-45fc-b7fa-7616bce1ab3f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('import done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e87250d5-7f85-409d-96fb-4225443dfd7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0          1       0       0       0       0       0       0       0       0   \n1          0       0       0       0       0       0       0       0       0   \n2          1       0       0       0       0       0       0       0       0   \n3          4       0       0       0       0       0       0       0       0   \n4          0       0       0       0       0       0       0       0       0   \n...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n41995      0       0       0       0       0       0       0       0       0   \n41996      1       0       0       0       0       0       0       0       0   \n41997      7       0       0       0       0       0       0       0       0   \n41998      6       0       0       0       0       0       0       0       0   \n41999      9       0       0       0       0       0       0       0       0   \n\n       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n0           0  ...         0         0         0         0         0   \n1           0  ...         0         0         0         0         0   \n2           0  ...         0         0         0         0         0   \n3           0  ...         0         0         0         0         0   \n4           0  ...         0         0         0         0         0   \n...       ...  ...       ...       ...       ...       ...       ...   \n41995       0  ...         0         0         0         0         0   \n41996       0  ...         0         0         0         0         0   \n41997       0  ...         0         0         0         0         0   \n41998       0  ...         0         0         0         0         0   \n41999       0  ...         0         0         0         0         0   \n\n       pixel779  pixel780  pixel781  pixel782  pixel783  \n0             0         0         0         0         0  \n1             0         0         0         0         0  \n2             0         0         0         0         0  \n3             0         0         0         0         0  \n4             0         0         0         0         0  \n...         ...       ...       ...       ...       ...  \n41995         0         0         0         0         0  \n41996         0         0         0         0         0  \n41997         0         0         0         0         0  \n41998         0         0         0         0         0  \n41999         0         0         0         0         0  \n\n[42000 rows x 785 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>41995</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41996</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41997</th>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41998</th>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41999</th>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>42000 rows Ã— 785 columns</p>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ef28b27-ca41-4c2f-8613-d48fd9ea9a1c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f90c361e0b0>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parent_data = np.array(data)\n",
    "np.random.shuffle(parent_data)\n",
    "\n",
    "X_labels, X_train = parent_data[:40000, :1].flatten(), parent_data[:40000, 1:].T /255\n",
    "Y_dev, X_dev = parent_data[40000:, :1].flatten(), parent_data[40000:, 1:].T /255\n",
    "\n",
    "plt.imshow(X_train[:, 0].reshape(28, 28), cmap='gray') # showing one image as sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d8c3d62d-f626-4605-8775-3c2c8b1d6354",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def init_wb(self, hidden_l: int, neurons: int):\n",
    "        const, lst_wb = 0.5, []\n",
    "        for i in range(hidden_l):\n",
    "            W = np.random.rand(neurons, neurons)\n",
    "            B = np.random.rand(neurons, 1)\n",
    "            \n",
    "            if i == 0:\n",
    "                W = np.random.rand(neurons, 784)\n",
    "                B = np.random.rand(neurons, 1)\n",
    "            \n",
    "            lst_wb.append([W - const, B - const])\n",
    "            \n",
    "        else:\n",
    "            W = np.random.rand(10, neurons)\n",
    "            B = np.random.rand(10, 1)\n",
    "            \n",
    "            lst_wb.append([W - const, B - const])\n",
    "        \n",
    "        return lst_wb\n",
    "    \n",
    "    def softmax_on_Y(self, Z):\n",
    "        return np.exp(Z)/sum(np.exp(Z))\n",
    "    \n",
    "    def RelU_activation(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def forward_propagation(self, lst_wb: list, X):\n",
    "        lst_za, prev_l = [], X\n",
    "        for i in range(len(lst_wb) - 1):\n",
    "            W, B = lst_wb[i][0], lst_wb[i][1]\n",
    "            \n",
    "            Z = W.dot(prev_l) + B\n",
    "            A = self.RelU_activation(Z)\n",
    "            \n",
    "            lst_za.append([Z, A])\n",
    "            prev_l = A\n",
    "            \n",
    "        else:\n",
    "            W_last, B_last = lst_wb[-1][0], lst_wb[-1][1]\n",
    "            prev_a = lst_za[-1][1]\n",
    "            \n",
    "            Z = W_last.dot(prev_a) + B_last\n",
    "            A = self.softmax_on_Y(Z)\n",
    "            \n",
    "            lst_za.append([Z, A])\n",
    "        \n",
    "        return lst_za\n",
    "        \n",
    "    def RelU_derivation(self, Z):\n",
    "        return Z > 0\n",
    "    \n",
    "    def error_backpropagation(self, lst_za, lst_wb):\n",
    "        expected_Y, size = self.one_hot_Y()\n",
    "        softmax_activation = lst_za[-1][1]\n",
    "        \n",
    "        \n",
    "        cost_derivative = (softmax_activation - expected_Y) * 2/size # MSE\n",
    "        error_in_l, derivation_output = cost_derivative, []\n",
    "        \n",
    "        for index, each in enumerate(lst_za):\n",
    "            if index == len(lst_za) - 1:\n",
    "                prev_activation = X_train\n",
    "            else:\n",
    "                prev_activation = lst_za[-(index + 2)][1]\n",
    "            \n",
    "            dW = error_in_l.dot(prev_activation.T) \n",
    "            dB = np.sum(error_in_l) \n",
    "            derivation_output.append([dW, dB])\n",
    "            \n",
    "            if index != len(lst_za) - 1:\n",
    "                W, Z = lst_wb[-(index + 1)][0], lst_za[-(index + 2)][0]\n",
    "                error_in_l = W.T.dot(error_in_l) * self.RelU_derivation(Z)\n",
    "        \n",
    "        derivation_output.reverse()\n",
    "        return derivation_output\n",
    "                \n",
    "    def one_hot_Y(self):\n",
    "        size = X_train.shape[1]\n",
    "        arr = np.zeros((size, 10))\n",
    "        arr[list(range(size)), X_labels] = 1\n",
    "        \n",
    "        return arr.T, size\n",
    "    \n",
    "    def updating_wb(self, alpha, lst_wb, derivation_output):\n",
    "        for index, each in enumerate(lst_wb):\n",
    "            W = each[0]\n",
    "            B = each[1]\n",
    "            \n",
    "            lst_wb[index][0] = W - alpha * derivation_output[index][0]\n",
    "            lst_wb[index][1] = B - alpha * derivation_output[index][1]\n",
    "            \n",
    "        return lst_wb\n",
    "    \n",
    "    def prediction_accur(self, softmax_Y):\n",
    "        indices_output = np.argmax(softmax_Y, axis=0)\n",
    "        percentage = np.sum(indices_output == X_labels)/400\n",
    "        \n",
    "        return percentage, indices_output\n",
    "    \n",
    "    def store_final_in_txt(self, lst_wb:list):\n",
    "        import shutil, os\n",
    "        shutil.rmtree('learned_wb')\n",
    "        os.mkdir('learned_wb')\n",
    "        for index, each in enumerate(lst_wb):\n",
    "            W = each[0]\n",
    "            B = each[1]\n",
    "\n",
    "            file_w = f'learned_wb/{index}_W.txt'\n",
    "            file_b = f'learned_wb/{index}_WB.txt'\n",
    "\n",
    "            np.savetxt(file_w, W)\n",
    "            np.savetxt(file_b, B)\n",
    "            \n",
    "    def gradient_descent(self, iteration, learning_rate, neurons, hidden_l):\n",
    "        randomized_wb = self.init_wb(hidden_l, neurons)\n",
    "        lst_wb = randomized_wb\n",
    "        for i in range(1, iteration + 1):\n",
    "            lst_za = self.forward_propagation(lst_wb, X_train)\n",
    "            derivation_output = self.error_backpropagation(lst_za, lst_wb)\n",
    "            \n",
    "            updated_wb = self.updating_wb(learning_rate, lst_wb, derivation_output)\n",
    "            lst_wb = updated_wb\n",
    "            \n",
    "            print(f'iteration: {i}')\n",
    "            percentage, indices_output = self.prediction_accur(lst_za[-1][1])\n",
    "            loss = np.sum(np.square(np.subtract(lst_za[-1][1], neural_network.one_hot_Y()[0]))) / 40000\n",
    "            print('Actual Output:', X_labels, 'Prediction Output:', indices_output)\n",
    "            print(f'predicted percentage correctness: {percentage}', f'Loss: {loss}\\n')\n",
    "        \n",
    "        self.store_final_in_txt(lst_wb)\n",
    "        return lst_wb, lst_za"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43ca80ba-6ec3-407c-8819-a641928b28b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [9 9 9 ... 9 9 9]\n",
      "predicted percentage correctness: 10.46 Loss: 1.7288985952557963\n",
      "\n",
      "iteration: 2\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [9 9 9 ... 9 0 9]\n",
      "predicted percentage correctness: 10.3975 Loss: 1.6911255114395165\n",
      "\n",
      "iteration: 3\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [9 9 9 ... 9 0 7]\n",
      "predicted percentage correctness: 10.7175 Loss: 1.6451213795261044\n",
      "\n",
      "iteration: 4\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [9 9 9 ... 5 0 7]\n",
      "predicted percentage correctness: 11.105 Loss: 1.6132297725210059\n",
      "\n",
      "iteration: 5\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [9 9 9 ... 5 0 7]\n",
      "predicted percentage correctness: 11.415 Loss: 1.5961451348108664\n",
      "\n",
      "iteration: 6\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [9 9 9 ... 5 0 7]\n",
      "predicted percentage correctness: 11.5575 Loss: 1.5853057138630697\n",
      "\n",
      "iteration: 7\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [9 9 9 ... 5 0 7]\n",
      "predicted percentage correctness: 11.7125 Loss: 1.5783138340298077\n",
      "\n",
      "iteration: 8\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [9 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 11.795 Loss: 1.573486103015868\n",
      "\n",
      "iteration: 9\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [9 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 11.8025 Loss: 1.569698066168143\n",
      "\n",
      "iteration: 10\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [9 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 11.85 Loss: 1.5663409321803228\n",
      "\n",
      "iteration: 11\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [9 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 11.92 Loss: 1.5630555432150866\n",
      "\n",
      "iteration: 12\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [9 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 11.9675 Loss: 1.5598752564996705\n",
      "\n",
      "iteration: 13\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [9 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 11.99 Loss: 1.5568796685012933\n",
      "\n",
      "iteration: 14\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 12.065 Loss: 1.5539510299566524\n",
      "\n",
      "iteration: 15\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 12.135 Loss: 1.5509707016457153\n",
      "\n",
      "iteration: 16\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 12.2625 Loss: 1.5479539785716412\n",
      "\n",
      "iteration: 17\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 12.3 Loss: 1.5448753611537074\n",
      "\n",
      "iteration: 18\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 12.4525 Loss: 1.541674324115344\n",
      "\n",
      "iteration: 19\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [0 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 12.57 Loss: 1.5383354976053663\n",
      "\n",
      "iteration: 20\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [0 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 12.64 Loss: 1.5348544158640927\n",
      "\n",
      "iteration: 21\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [0 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 12.77 Loss: 1.5312232915950403\n",
      "\n",
      "iteration: 22\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [0 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 12.9075 Loss: 1.5274432966156979\n",
      "\n",
      "iteration: 23\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [0 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 13.04 Loss: 1.5235422027890775\n",
      "\n",
      "iteration: 24\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [0 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 13.2075 Loss: 1.519547287861878\n",
      "\n",
      "iteration: 25\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [0 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 13.325 Loss: 1.5154841841840467\n",
      "\n",
      "iteration: 26\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [0 3 9 ... 5 0 7]\n",
      "predicted percentage correctness: 13.5475 Loss: 1.5113706774155569\n",
      "\n",
      "iteration: 27\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 13.6925 Loss: 1.507212793223688\n",
      "\n",
      "iteration: 28\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 13.86 Loss: 1.5030091476526835\n",
      "\n",
      "iteration: 29\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 14.1 Loss: 1.4987560738045296\n",
      "\n",
      "iteration: 30\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 14.2825 Loss: 1.49445390947143\n",
      "\n",
      "iteration: 31\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 14.4875 Loss: 1.4901057380604508\n",
      "\n",
      "iteration: 32\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 14.6775 Loss: 1.485714389130058\n",
      "\n",
      "iteration: 33\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 14.875 Loss: 1.4812808678229106\n",
      "\n",
      "iteration: 34\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 15.0275 Loss: 1.4768033175444717\n",
      "\n",
      "iteration: 35\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 15.165 Loss: 1.4722912309007707\n",
      "\n",
      "iteration: 36\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 15.375 Loss: 1.467744062753093\n",
      "\n",
      "iteration: 37\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 15.5925 Loss: 1.4631671095303171\n",
      "\n",
      "iteration: 38\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 15.755 Loss: 1.4585645980354631\n",
      "\n",
      "iteration: 39\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 15.96 Loss: 1.4539367856253367\n",
      "\n",
      "iteration: 40\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 16.18 Loss: 1.4492793730639184\n",
      "\n",
      "iteration: 41\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 16.41 Loss: 1.4445963320309472\n",
      "\n",
      "iteration: 42\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 16.6325 Loss: 1.439893454043045\n",
      "\n",
      "iteration: 43\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 16.82 Loss: 1.4351738134185834\n",
      "\n",
      "iteration: 44\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 16.995 Loss: 1.4304435600961394\n",
      "\n",
      "iteration: 45\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 5 0 7]\n",
      "predicted percentage correctness: 17.185 Loss: 1.4256973490150777\n",
      "\n",
      "iteration: 46\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 2 0 7]\n",
      "predicted percentage correctness: 17.395 Loss: 1.4209438951482378\n",
      "\n",
      "iteration: 47\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 2 0 7]\n",
      "predicted percentage correctness: 17.585 Loss: 1.4161876447397492\n",
      "\n",
      "iteration: 48\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 2 0 7]\n",
      "predicted percentage correctness: 17.7875 Loss: 1.4114254306892575\n",
      "\n",
      "iteration: 49\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 2 0 7]\n",
      "predicted percentage correctness: 17.9925 Loss: 1.4066582962429097\n",
      "\n",
      "iteration: 50\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 2 0 7]\n",
      "predicted percentage correctness: 18.2 Loss: 1.401888689759747\n",
      "\n",
      "iteration: 51\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 2 0 7]\n",
      "predicted percentage correctness: 18.445 Loss: 1.3971149063332715\n",
      "\n",
      "iteration: 52\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 2 0 7]\n",
      "predicted percentage correctness: 18.655 Loss: 1.3923432549626897\n",
      "\n",
      "iteration: 53\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 2 0 7]\n",
      "predicted percentage correctness: 18.8425 Loss: 1.3875694429251326\n",
      "\n",
      "iteration: 54\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 2 0 6]\n",
      "predicted percentage correctness: 19.065 Loss: 1.3827951893781145\n",
      "\n",
      "iteration: 55\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 2 0 6]\n",
      "predicted percentage correctness: 19.255 Loss: 1.3780212678699872\n",
      "\n",
      "iteration: 56\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 2 0 6]\n",
      "predicted percentage correctness: 19.4225 Loss: 1.3732478664166874\n",
      "\n",
      "iteration: 57\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 2 0 6]\n",
      "predicted percentage correctness: 19.67 Loss: 1.3684787633106261\n",
      "\n",
      "iteration: 58\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 2 0 6]\n",
      "predicted percentage correctness: 19.9175 Loss: 1.3637095007346736\n",
      "\n",
      "iteration: 59\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 2 0 6]\n",
      "predicted percentage correctness: 20.17 Loss: 1.358945869390304\n",
      "\n",
      "iteration: 60\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 2 0 6]\n",
      "predicted percentage correctness: 20.4 Loss: 1.3541819003734652\n",
      "\n",
      "iteration: 61\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 1 0 6]\n",
      "predicted percentage correctness: 20.6175 Loss: 1.3494180824358337\n",
      "\n",
      "iteration: 62\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 1 0 6]\n",
      "predicted percentage correctness: 20.8575 Loss: 1.3446583362627638\n",
      "\n",
      "iteration: 63\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [6 3 3 ... 1 0 6]\n",
      "predicted percentage correctness: 21.09 Loss: 1.3399018408025445\n",
      "\n",
      "iteration: 64\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 3 3 ... 1 0 6]\n",
      "predicted percentage correctness: 21.275 Loss: 1.3351492824080486\n",
      "\n",
      "iteration: 65\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 3 3 ... 1 0 6]\n",
      "predicted percentage correctness: 21.48 Loss: 1.3304039067509739\n",
      "\n",
      "iteration: 66\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 3 3 ... 1 0 6]\n",
      "predicted percentage correctness: 21.6975 Loss: 1.3256636082123405\n",
      "\n",
      "iteration: 67\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 3 3 ... 1 0 6]\n",
      "predicted percentage correctness: 21.84 Loss: 1.3209265484105754\n",
      "\n",
      "iteration: 68\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 3 3 ... 1 0 6]\n",
      "predicted percentage correctness: 22.0675 Loss: 1.3161961380506348\n",
      "\n",
      "iteration: 69\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 3 3 ... 1 0 6]\n",
      "predicted percentage correctness: 22.3525 Loss: 1.3114706263035347\n",
      "\n",
      "iteration: 70\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 3 ... 1 0 6]\n",
      "predicted percentage correctness: 22.59 Loss: 1.3067502762563687\n",
      "\n",
      "iteration: 71\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 3 ... 1 0 6]\n",
      "predicted percentage correctness: 22.82 Loss: 1.3020349307619021\n",
      "\n",
      "iteration: 72\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 3 ... 1 0 6]\n",
      "predicted percentage correctness: 23.06 Loss: 1.2973245926138015\n",
      "\n",
      "iteration: 73\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 3 ... 1 0 6]\n",
      "predicted percentage correctness: 23.3175 Loss: 1.2926245754117307\n",
      "\n",
      "iteration: 74\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 3 ... 1 0 6]\n",
      "predicted percentage correctness: 23.535 Loss: 1.2879329541412556\n",
      "\n",
      "iteration: 75\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 3 ... 1 0 6]\n",
      "predicted percentage correctness: 23.7375 Loss: 1.2832521703861803\n",
      "\n",
      "iteration: 76\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 3 ... 1 0 6]\n",
      "predicted percentage correctness: 23.955 Loss: 1.2785806976259932\n",
      "\n",
      "iteration: 77\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 3 ... 1 0 6]\n",
      "predicted percentage correctness: 24.1825 Loss: 1.2739223154385237\n",
      "\n",
      "iteration: 78\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 3 ... 1 0 6]\n",
      "predicted percentage correctness: 24.435 Loss: 1.2692795778910024\n",
      "\n",
      "iteration: 79\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 24.695 Loss: 1.2646509635697327\n",
      "\n",
      "iteration: 80\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 24.8875 Loss: 1.2600374636389715\n",
      "\n",
      "iteration: 81\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 25.145 Loss: 1.255439315203158\n",
      "\n",
      "iteration: 82\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 25.34 Loss: 1.2508556816647793\n",
      "\n",
      "iteration: 83\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 25.57 Loss: 1.2462878360683871\n",
      "\n",
      "iteration: 84\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 25.7975 Loss: 1.2417376804621951\n",
      "\n",
      "iteration: 85\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 26.04 Loss: 1.2372018750364324\n",
      "\n",
      "iteration: 86\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 26.2875 Loss: 1.2326832569312118\n",
      "\n",
      "iteration: 87\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 26.51 Loss: 1.2281818397058315\n",
      "\n",
      "iteration: 88\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 26.69 Loss: 1.223694317988353\n",
      "\n",
      "iteration: 89\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 26.9175 Loss: 1.2192208611594078\n",
      "\n",
      "iteration: 90\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 27.145 Loss: 1.2147631083647423\n",
      "\n",
      "iteration: 91\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 27.3725 Loss: 1.2103218370565645\n",
      "\n",
      "iteration: 92\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 27.56 Loss: 1.2058990199818298\n",
      "\n",
      "iteration: 93\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 27.77 Loss: 1.2014910105045513\n",
      "\n",
      "iteration: 94\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 27.985 Loss: 1.1971020793111211\n",
      "\n",
      "iteration: 95\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 28.2575 Loss: 1.192731177977529\n",
      "\n",
      "iteration: 96\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 28.4625 Loss: 1.1883755814315795\n",
      "\n",
      "iteration: 97\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 28.68 Loss: 1.1840374342708722\n",
      "\n",
      "iteration: 98\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 28.9125 Loss: 1.179716397050061\n",
      "\n",
      "iteration: 99\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 29.125 Loss: 1.175411556495794\n",
      "\n",
      "iteration: 100\n",
      "Actual Output: [3 6 0 ... 1 6 3] Prediction Output: [7 6 0 ... 1 0 6]\n",
      "predicted percentage correctness: 29.3675 Loss: 1.171126581462275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    neural_network = NeuralNetwork()\n",
    "    lst_wb, lst_za = neural_network.gradient_descent(100, .001, 128, 2)\n",
    "    softma = neural_network.forward_propagation(lst_wb, X_dev)[-1][1]\n",
    "    neural_network.store_final_in_txt(lst_wb=lst_wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9baf47c7-5710-482e-aaca-01bb3f3ebb0a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "821f751d-986e-4e58-be41-1a9389c60165",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}